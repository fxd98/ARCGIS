{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe41113b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6a5ed0",
   "metadata": {},
   "source": [
    "# 降雨数据转栅格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de5b603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading target file: data/grid1km_gd_rain_xls_2024\\grid1km_obs_rain_2024061523.xlsx\n",
      "Total unique hourly files needed: 72\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ----------------- USER PARAMETERS -----------------\n",
    "FOLDER = r'data/grid1km_gd_rain_xls_2024'  # updated folder path\n",
    "TARGET_STR = '2024061523'  # target timestamp in filenames YYYYMMDDHH\n",
    "output_path = r\"../out/rain_cumulative_2024061523.csv\"\n",
    "# ---------------------------------------------------\n",
    "\n",
    "WINDOWS = {\n",
    "    'rain_cum_3d': 72,\n",
    "    'rain_cum_2d': 48,\n",
    "    'rain_cum_1d': 24,\n",
    "    'rain_cum_2h': 2,\n",
    "    'rain_cum_3h': 3,\n",
    "    'rain_cum_4h': 4,\n",
    "    'rain_cum_5h': 5,\n",
    "    'rain_cum_6h': 6,\n",
    "    'rain_cum_7h': 7,\n",
    "}\n",
    "\n",
    "def parse_yyyymmddhh(s: str) -> datetime:\n",
    "    year = int(s[0:4]); month = int(s[4:6]); day = int(s[6:8]); hour = int(s[8:10])\n",
    "    if hour == 24:\n",
    "        dt = datetime(year, month, day) + timedelta(days=1)\n",
    "    else:\n",
    "        dt = datetime(year, month, day, hour)\n",
    "    return dt\n",
    "\n",
    "pattern = os.path.join(FOLDER, '*.xlsx')\n",
    "files = glob.glob(pattern)\n",
    "if not files:\n",
    "    raise SystemExit(f'No .xlsx files found in folder: {FOLDER}')\n",
    "\n",
    "file_dt_map = {}\n",
    "filename_re = re.compile(r'(\\d{10})')\n",
    "for fp in files:\n",
    "    bn = os.path.basename(fp)\n",
    "    m = filename_re.search(bn)\n",
    "    if not m:\n",
    "        continue\n",
    "    ts_str = m.group(1)\n",
    "    try:\n",
    "        dt = parse_yyyymmddhh(ts_str)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: couldn't parse timestamp from {bn}: {e}\")\n",
    "        continue\n",
    "    file_dt_map[dt] = fp\n",
    "\n",
    "target_dt = parse_yyyymmddhh(TARGET_STR)\n",
    "if target_dt not in file_dt_map:\n",
    "    raise SystemExit(f\"Target file for {TARGET_STR} not found in folder.\")\n",
    "\n",
    "target_fp = file_dt_map[target_dt]\n",
    "print('Loading target file:', target_fp)\n",
    "df_target = pd.read_excel(target_fp, engine='openpyxl')\n",
    "required_cols = ['URI', 'lon', 'lat', 'province', 'city', 'Rain']\n",
    "missing_cols = [c for c in required_cols if c not in df_target.columns]\n",
    "if missing_cols:\n",
    "    raise SystemExit(f'Target file missing columns: {missing_cols}')\n",
    "\n",
    "base = df_target[['URI','lon','lat','province']].drop_duplicates(subset=['URI']).set_index('URI')\n",
    "base = base.sort_index()\n",
    "\n",
    "series_by_dt = {}\n",
    "needed_dts = set()\n",
    "for hours in WINDOWS.values():\n",
    "    for h in range(1, hours+1):\n",
    "        needed_dts.add(target_dt - timedelta(hours=h))\n",
    "needed_dts = sorted(needed_dts)\n",
    "\n",
    "print(f'Total unique hourly files needed: {len(needed_dts)}')\n",
    "\n",
    "for dt in needed_dts:\n",
    "    if dt in file_dt_map:\n",
    "        fp = file_dt_map[dt]\n",
    "        try:\n",
    "            dfi = pd.read_excel(fp, engine='openpyxl')\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: failed to read {fp}: {e}.\")\n",
    "            series_by_dt[dt] = pd.Series(index=base.index, data=float('nan'))\n",
    "            continue\n",
    "        if 'URI' not in dfi.columns or 'Rain' not in dfi.columns:\n",
    "            print(f\"Warning: file {fp} missing URI or Rain columns.\")\n",
    "            series_by_dt[dt] = pd.Series(index=base.index, data=float('nan'))\n",
    "            continue\n",
    "        s = dfi.set_index('URI')['Rain'].reindex(base.index)\n",
    "        series_by_dt[dt] = s\n",
    "    else:\n",
    "        print(f\"Warning: hourly file for {dt} not found.\")\n",
    "        series_by_dt[dt] = pd.Series(index=base.index, data=float('nan'))\n",
    "\n",
    "out = base.copy()\n",
    "for colname, hours in WINDOWS.items():\n",
    "    dts = [target_dt - timedelta(hours=h) for h in range(1, hours+1)]\n",
    "    frames = [series_by_dt[d] for d in dts]\n",
    "    if frames:\n",
    "        df_concat = pd.concat(frames, axis=1)\n",
    "        out[colname] = df_concat.sum(axis=1, skipna=True, min_count=1)\n",
    "    else:\n",
    "        out[colname] = float('nan')\n",
    "\n",
    "out_reset = out.reset_index()\n",
    "keep_cols = ['URI','lon','lat','province'] + list(WINDOWS.keys())\n",
    "out_final = out_reset[keep_cols]\n",
    "df = out_final\n",
    "base_cols = ['URI', 'lon', 'lat', 'province']\n",
    "rain_cols = [col for col in df.columns if col not in base_cols]\n",
    "df_no_neg = df[(df[rain_cols] >= 0).all(axis=1)]\n",
    "df_no_neg.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504ff0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入 CSV\n",
    "csv_path = r\"../out/rain_cumulative_2024061523.csv\"\n",
    "\n",
    "# 输出文件夹\n",
    "out_dir = r\"../out/grid1km_gd_rain_xls_2024/rain_rasters\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# 读取数据\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# 基础列\n",
    "base_cols = ['URI', 'lon', 'lat', 'province']\n",
    "\n",
    "# 累计降雨量列\n",
    "rain_cols = [col for col in df.columns if col not in base_cols]\n",
    "\n",
    "# 获取唯一经纬度并排序\n",
    "lons = np.sort(df['lon'].unique())\n",
    "lats = np.sort(df['lat'].unique())[::-1]  # 从北到南\n",
    "\n",
    "# 分辨率（假设经纬度为度）\n",
    "res_x = np.abs(lons[1] - lons[0])\n",
    "res_y = np.abs(lats[1] - lats[0])\n",
    "\n",
    "# 创建 transform\n",
    "transform = from_origin(west=lons.min() - res_x/2,\n",
    "                        north=lats.max() + res_y/2,\n",
    "                        xsize=res_x,\n",
    "                        ysize=res_y)\n",
    "\n",
    "# 行列数\n",
    "height = len(lats)\n",
    "width = len(lons)\n",
    "\n",
    "# 对每个累计降雨量列生成栅格\n",
    "for col in rain_cols:\n",
    "    # 初始化空阵\n",
    "    arr = np.full((height, width), np.nan, dtype=np.float32)\n",
    "\n",
    "    # 填值\n",
    "    for _, row in df.iterrows():\n",
    "        x_idx = np.where(lons == row['lon'])[0][0]\n",
    "        y_idx = np.where(lats == row['lat'])[0][0]\n",
    "        arr[y_idx, x_idx] = row[col]\n",
    "\n",
    "    # 输出路径\n",
    "    out_path = os.path.join(out_dir, f\"{col}_2024061523.tif\")\n",
    "\n",
    "    # 保存 GeoTIFF\n",
    "    with rasterio.open(\n",
    "        out_path,\n",
    "        'w',\n",
    "        driver='GTiff',\n",
    "        height=height,\n",
    "        width=width,\n",
    "        count=1,\n",
    "        dtype=arr.dtype,\n",
    "        crs='EPSG:4326',  # WGS84\n",
    "        transform=transform,\n",
    "        nodata=np.nan\n",
    "    ) as dst:\n",
    "        dst.write(arr, 1)\n",
    "\n",
    "    print(f\"已生成: {out_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
